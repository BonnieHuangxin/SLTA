# Cross-Modal Video Moment Retrieval with Spatial and Language-Temporal Attention
This is our implementation for the paper:

Bin Jiang, Xin Huang, Chao Yang and Junsong Yuan. 2019. Cross-Modal Video Moment Retrieval with Spatial and Language-Temporal Attention.  In <em>Proceedings of the 2019 ACM on International Conference on Multimedia Retrieval. &#38; 

In order to localize the temporal segment within the video that best describes the textual query, we propose a SLTA (short for
“Spatial and Language-Temporal Attention”) method that can adaptively recognize the most relevant objects and interactions in the video, and simultaneously highlight the keywords in the query for retrieving the desired moment.

**Please cite our ICMR'18 paper if you use our codes. Thanks!** 


